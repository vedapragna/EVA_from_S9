# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-kRMvh9DhJq8orABT0cRcJIVMYayBB46
"""

import torch.nn as nn
import torch.nn.functional as F

dropout_value = 0.1

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        
        # Input Block
        self.convblock1 = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3, 3),dilation= 1,  padding=0, bias=False),
            nn.ReLU()
            ,nn.BatchNorm2d(32)
            #,nn.Dropout(dropout_value)
        ) # output_size = 30


        # CONVOLUTION BLOCK 1
        self.convblock2 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=128, kernel_size=(3, 3), padding=1, groups = 32, bias=False), #16
            nn.Conv2d(in_channels=128, out_channels=32, kernel_size=(1, 1), padding=0, bias=False) #18
            ,nn.ReLU()         
            ,nn.BatchNorm2d(32)
            #,nn.Dropout(dropout_value)
        ) # output_size = 30
        
        self.convblock2_1 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=128, kernel_size=(3, 3), padding=1, groups = 32, bias=False), #16
            nn.Conv2d(in_channels=128, out_channels=32, kernel_size=(1, 1), padding=0, bias=False) #18
            ,nn.ReLU()         
            ,nn.BatchNorm2d(32)
            #,nn.Dropout(dropout_value)
        ) # output_size = 30
        # TRANSITION BLOCK 1
        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 14


        # CONVOLUTION BLOCK 2
        self.convblock3 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=128, kernel_size=(3, 3), padding=1, groups = 32, bias=False), #16
            nn.Conv2d(in_channels=128, out_channels=32, kernel_size=(1, 1), padding=0, bias=False) #18
            ,nn.ReLU()         
            ,nn.BatchNorm2d(32)
            #,nn.Dropout(dropout_value)
        ) # output_size = 16
        
        # CONVOLUTION BLOCK 3
        self.convblock4 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=128, kernel_size=(3, 3), padding=1, groups = 32, bias=False), #16
            nn.Conv2d(in_channels=128, out_channels=32, kernel_size=(1, 1), padding=0, bias=False) #18
            ,nn.ReLU()         
            ,nn.BatchNorm2d(32)
           # ,nn.Dropout(dropout_value)
        ) # output_size = 16
        self.convblock4_t = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=128, kernel_size=(3, 3), padding=1, groups = 32, bias=False), #16
            nn.Conv2d(in_channels=128, out_channels=32, kernel_size=(1, 1), padding=0, bias=False) #18
            ,nn.ReLU()         
            ,nn.BatchNorm2d(32)
            #,nn.Dropout(dropout_value)
        )
        # nn.Sequential(
        #     nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(1, 1), padding=0, bias=False),
        # ) # output_size = 18
        self.pool3 = nn.MaxPool2d(2, 2) # output_size = 9


        # OUTPUT BLOCK
        self.convblock5 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=128, kernel_size=(3, 3), padding=1, groups = 32, bias=False), #16
            nn.Conv2d(in_channels=128, out_channels=32, kernel_size=(1, 1), padding=0, bias=False) #18
            ,nn.ReLU()         
            ,nn.BatchNorm2d(32)
           # ,nn.Dropout(dropout_value)
        )# output_size = 7

        self.convblock6 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=128, kernel_size=(3, 3), padding=1, groups = 32, bias=False), #16
            nn.Conv2d(in_channels=128, out_channels=32, kernel_size=(1, 1), padding=0, bias=False) #18
            ,nn.ReLU()         
            ,nn.BatchNorm2d(32)
           # ,nn.Dropout(dropout_value)
        )# output_size = 5
        self.convblock6_t = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(1, 1), padding=0, bias=False),
        )# output_size = 5
        self.gap = nn.Sequential(
            nn.AvgPool2d(kernel_size=7)
        ) # output_size = 1

        self.linear = nn.Linear(1568,10) #1568

        self.dropout = nn.Dropout(dropout_value)

    def forward(self, x):
        x1 = self.convblock1(x)
        # print(x1.shape)
        x2 = self.convblock2(x1)
        # print(x2.shape)
        x3 = self.convblock2_1(x1+x2)
        # print(x3.shape)
        x4 = self.pool1(x1+x2+x3)
        # print(x4.shape)

        x5 = self.convblock3(x4)
        # print(x5.shape)
        x6 = self.convblock4(x4+x5)
        x7 = self.convblock4_t(x4+x5+x6)
        # print(x6.shape)
        # print(x7.shape)
        x8 = self.pool3(x5+x6+x7)

        x9 = self.convblock5(x8)
        #print("pool3: ",x8.shape)
        #print("conv5: ",x9.shape)
        x10 = self.convblock6(x8 + x9)
        #print("conv6: ",x10.shape)
        x11 = self.convblock6_t(x8 + x9 + x10)
        #print("conv6_t: ",x11.shape)
        x12 = self.gap(x11)   
        #print("gap: ",x12.shape)
        x12 = x11.view(x11.size(0), -1)
        #print("Flatten: ",x12.shape)
        x13 = self.linear(x12)     
        # x = self.convblock5(x)
        #print("linear: ",x13.shape)
        # x = x.view(-1, 10)
        return x13#F.log_softmax(x, dim=-1)